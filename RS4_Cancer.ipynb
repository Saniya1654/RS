{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HTieKNLbywBG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "63f-_BoQ_iJR"
      },
      "outputs": [],
      "source": [
        "# ---------------------- Configuration ----------------------\n",
        "CSV_PATH = 'RS-A4_SEER Breast Cancer Dataset .csv' # change if necessary\n",
        "TARGET_COL = 'Status' # column to predict (alive / dead)\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "MODEL_OUTPUT = 'bc_prognosis_model.joblib'\n",
        "PIPE_OUTPUT = 'bc_preproc_pipe.joblib'\n",
        "PROB_THRESHOLD = 0.5 # threshold for \"high risk\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P4eoy8Mg_iSO"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "  df = pd.read_csv(path)\n",
        "  print('Loaded:', df.shape)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CmR70FX9_iZx"
      },
      "outputs": [],
      "source": [
        "def standardize_target(df, target_col):\n",
        "    # Convert textual status to numeric 1=alive, 0=dead (or the other way around depending on your preference)\n",
        "    # We'll convert to 1 = alive (good), 0 = dead (bad). If user already has 0/1, this will be left as is.\n",
        "    if df[target_col].dtype == object:\n",
        "        mapping = {k.lower(): v for k, v in zip(['alive', 'dead'], [1, 0])}\n",
        "        # attempt mapping robustly\n",
        "        df[target_col] = df[target_col].astype(str).str.strip().str.lower().map({'alive':1, 'dead':0})\n",
        "        # if mapping produced NaNs but values are like '0'/'1'\n",
        "        if df[target_col].isna().any():\n",
        "            try:\n",
        "                df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n",
        "            except Exception:\n",
        "                pass\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iG4o4pGY_ig1"
      },
      "outputs": [],
      "source": [
        "# ---------------------- Main pipeline builder ----------------------\n",
        "\n",
        "def build_and_train(df, target_col=TARGET_COL):\n",
        "    df = df.copy()\n",
        "    df = standardize_target(df, target_col)\n",
        "\n",
        "    # Drop rows where target is still NaN\n",
        "    df = df.dropna(subset=[target_col])\n",
        "\n",
        "    # Identify feature columns\n",
        "    feature_cols = [c for c in df.columns if c != target_col]\n",
        "\n",
        "    # Simple heuristic: numeric vs categorical\n",
        "    numeric_cols = df[feature_cols].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_cols = [c for c in feature_cols if c not in numeric_cols]\n",
        "\n",
        "    # Build preprocessing\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "    clf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "\n",
        "    pipe = Pipeline(steps=[('preproc', preprocessor), ('clf', clf)])\n",
        "\n",
        "    # train-test split\n",
        "    X = df[feature_cols]\n",
        "    y = df[target_col].astype(int)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE,\n",
        "                                                        stratify=y, random_state=RANDOM_STATE)\n",
        "\n",
        "    # Optional: quick grid search for n_estimators/depth (small grid)\n",
        "    # grid = {'clf__n_estimators':[100,200], 'clf__max_depth':[None,10,20]}\n",
        "    # search = GridSearchCV(pipe, grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
        "    # search.fit(X_train, y_train)\n",
        "    # model = search.best_estimator_\n",
        "\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluation\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    y_proba = pipe.predict_proba(X_test)[:, 1] if hasattr(pipe, 'predict_proba') else None\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print('Accuracy on test set: {:.2f}%'.format(acc * 100))\n",
        "    print('\\nClassification Report:\\n', classification_report(y_test, y_pred, digits=4))\n",
        "    if y_proba is not None:\n",
        "        try:\n",
        "            auc = roc_auc_score(y_test, y_proba)\n",
        "            print('ROC AUC: {:.4f}'.format(auc))\n",
        "        except Exception:\n",
        "            pass\n",
        "    print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Save pipeline\n",
        "    joblib.dump(pipe, PIPE_OUTPUT)\n",
        "    print('Saved preprocessing + model pipeline to', PIPE_OUTPUT)\n",
        "\n",
        "    return pipe, numeric_cols, categorical_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gDrpIpYa_ioA"
      },
      "outputs": [],
      "source": [
        "# ---------------------- Recommendation function ----------------------\n",
        "\n",
        "def prognosis_recommendation(pipe, patient_row, numeric_cols, categorical_cols, prob_threshold=PROB_THRESHOLD):\n",
        "    \"\"\"\n",
        "    patient_row: pd.Series or 1D-array corresponding to feature columns (order: numeric_cols + categorical_cols)\n",
        "    Returns: dict with prediction, probability, risk_label, top_influential_features, recommendation_text\n",
        "    \"\"\"\n",
        "    # Build a DataFrame from patient_row\n",
        "    if isinstance(patient_row, (list, np.ndarray)):\n",
        "        # user provided numpy array; we need column names\n",
        "        cols = numeric_cols + categorical_cols\n",
        "        x = pd.DataFrame([patient_row], columns=cols)\n",
        "    elif isinstance(patient_row, pd.Series):\n",
        "        x = pd.DataFrame([patient_row.values], columns=patient_row.index)\n",
        "    elif isinstance(patient_row, pd.DataFrame):\n",
        "        x = patient_row\n",
        "    else:\n",
        "        raise ValueError('patient_row must be array-like, Series or DataFrame')\n",
        "\n",
        "    # Predict\n",
        "    proba = pipe.predict_proba(x)[:, 1][0] if hasattr(pipe, 'predict_proba') else None\n",
        "    pred = pipe.predict(x)[0]\n",
        "\n",
        "    # Decide risk: here 0=dead(bad),1=alive(good). We want to say high risk if probability of 'alive' < threshold.\n",
        "    # But because classes might be reversed, we'll check mapping via label ordering\n",
        "    classes = pipe.named_steps['clf'].classes_\n",
        "    # index of class 'alive' (1) if present\n",
        "    if 1 in classes:\n",
        "        alive_index = list(classes).index(1)\n",
        "        prob_alive = pipe.predict_proba(x)[:, alive_index][0]\n",
        "    else:\n",
        "        # fallback: use positive class probability as returned\n",
        "        prob_alive = proba\n",
        "\n",
        "    risk_label = 'High risk' if prob_alive < prob_threshold else 'Low/Moderate risk'\n",
        "\n",
        "    # Simple feature importance extraction -- map back to original feature names\n",
        "    # Works only for tree-based models\n",
        "    feat_info = []\n",
        "    try:\n",
        "        import numpy as _np\n",
        "        clf = pipe.named_steps['clf']\n",
        "        # get feature names after preprocessor\n",
        "        ohe = pipe.named_steps['preproc'].named_transformers_['cat'].named_steps['onehot']\n",
        "        cat_ohe_cols = list(ohe.get_feature_names_out(categorical_cols)) if hasattr(ohe, 'get_feature_names_out') else []\n",
        "        feat_names = numeric_cols + cat_ohe_cols\n",
        "        importances = clf.feature_importances_\n",
        "        imp_df = pd.DataFrame({'feature': feat_names, 'importance': importances})\n",
        "        imp_df = imp_df.sort_values('importance', ascending=False).head(6)\n",
        "        feat_info = imp_df.values.tolist()\n",
        "    except Exception:\n",
        "        feat_info = []\n",
        "\n",
        "    # Compose plain-language recommendation\n",
        "    if prob_alive is None:\n",
        "        recommendation = 'Model did not provide probabilities. Review model capability.'\n",
        "    elif prob_alive < 0.2:\n",
        "        recommendation = 'âš ï¸ Very high risk. Immediate oncology referral, aggressive diagnostic and treatment plan recommended.'\n",
        "    elif prob_alive < 0.5:\n",
        "        recommendation = 'âš ï¸ High risk. Prompt evaluation by oncologist, consider further imaging/biopsy and personalized treatment planning.'\n",
        "    elif prob_alive < 0.8:\n",
        "        recommendation = 'ðŸ” Moderate risk. Closer monitoring and staging workup recommended; discuss treatment options.'\n",
        "    else:\n",
        "        recommendation = 'âœ… Low risk. Routine follow-up and standard of care monitoring recommended.'\n",
        "\n",
        "    return {\n",
        "        'predicted_class': int(pred),\n",
        "        'probability_alive': float(prob_alive) if prob_alive is not None else None,\n",
        "        'risk_label': risk_label,\n",
        "        'top_influential': feat_info,\n",
        "        'recommendation': recommendation\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-YHHxJd_iwV",
        "outputId": "0576b349-11f9-4e2c-8232-31c67663ab6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded: (4024, 16)\n",
            "   Age                                              Race   \\\n",
            "0   43  Other (American Indian/AK Native, Asian/Pacifi...   \n",
            "1   47  Other (American Indian/AK Native, Asian/Pacifi...   \n",
            "2   67                                              White   \n",
            "3   46                                              White   \n",
            "4   63                                              White   \n",
            "\n",
            "                   Marital Status  Unnamed: 3 T Stage  N Stage 6th Stage  \\\n",
            "0  Married (including common law)         NaN       T2      N3      IIIC   \n",
            "1  Married (including common law)         NaN       T2      N2      IIIA   \n",
            "2  Married (including common law)         NaN       T2      N1       IIB   \n",
            "3                        Divorced         NaN       T1      N1       IIA   \n",
            "4  Married (including common law)         NaN       T2      N2      IIIA   \n",
            "\n",
            "                                 Grade   A Stage  Tumor Size Estrogen Status  \\\n",
            "0  Moderately differentiated; Grade II  Regional          40        Positive   \n",
            "1  Moderately differentiated; Grade II  Regional          45        Positive   \n",
            "2     Poorly differentiated; Grade III  Regional          25        Positive   \n",
            "3  Moderately differentiated; Grade II  Regional          19        Positive   \n",
            "4  Moderately differentiated; Grade II  Regional          35        Positive   \n",
            "\n",
            "  Progesterone Status  Regional Node Examined  Reginol Node Positive  \\\n",
            "0            Positive                      19                     11   \n",
            "1            Positive                      25                      9   \n",
            "2            Positive                       4                      1   \n",
            "3            Positive                      26                      1   \n",
            "4            Positive                      21                      5   \n",
            "\n",
            "   Survival Months Status  \n",
            "0                1  Alive  \n",
            "1                2  Alive  \n",
            "2                2   Dead  \n",
            "3                2   Dead  \n",
            "4                3   Dead  \n",
            "Accuracy on test set: 89.44%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7436    0.4715    0.5771       123\n",
            "           1     0.9106    0.9707    0.9397       682\n",
            "\n",
            "    accuracy                         0.8944       805\n",
            "   macro avg     0.8271    0.7211    0.7584       805\n",
            "weighted avg     0.8851    0.8944    0.8843       805\n",
            "\n",
            "ROC AUC: 0.8496\n",
            "Confusion Matrix:\n",
            " [[ 58  65]\n",
            " [ 20 662]]\n",
            "Saved preprocessing + model pipeline to bc_preproc_pipe.joblib\n",
            "\n",
            "Example patient actual status: Alive\n",
            "Recommendation output:\n",
            "{'predicted_class': 1, 'probability_alive': 0.92, 'risk_label': 'Low/Moderate risk', 'top_influential': [], 'recommendation': 'âœ… Low risk. Routine follow-up and standard of care monitoring recommended.'}\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    df = load_data(CSV_PATH)\n",
        "\n",
        "    # quick sanity: print first rows\n",
        "    print(df.head())\n",
        "\n",
        "    # Train the model on provided CSV. This will also save the pipeline to disk.\n",
        "    pipe, numeric_cols, categorical_cols = build_and_train(df, target_col=TARGET_COL)\n",
        "\n",
        "    # Example: pick a row from the dataset to demonstrate recommendation\n",
        "    example_idx = 0\n",
        "    features = df.drop(columns=[TARGET_COL]).iloc[example_idx]\n",
        "    rec = prognosis_recommendation(pipe, features, numeric_cols, categorical_cols)\n",
        "    print('\\nExample patient actual status:', df[TARGET_COL].iloc[example_idx])\n",
        "    print('Recommendation output:')\n",
        "    print(rec)\n",
        "\n",
        "    # Save final model (already saved pipeline contains both preproc and model)\n",
        "    # joblib.dump(pipe, MODEL_OUTPUT)\n",
        "\n",
        "    print('\\nDone.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6xOacnf_i32"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Perfect ðŸ‘ â€” hereâ€™s a **clear explanation of all major functions and operations** your code performs, broken down step-by-step.\n",
        "This will help you explain the project logically during presentation or documentation.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  **Project Overview**\n",
        "\n",
        "**Title:** Machine Learningâ€“Based Recommendation System for Breast Cancer Prognosis\n",
        "**Goal:** Predict the survival status (alive/dead) of breast cancer patients and provide personalized prognosis recommendations based on patient data.\n",
        "\n",
        "---\n",
        "\n",
        "## âš™ï¸ **Main Functional Components**\n",
        "\n",
        "### **1. `load_data(path)`**\n",
        "\n",
        "**Purpose:**\n",
        "Loads the dataset (`breast_cancer.csv`) into a Pandas DataFrame.\n",
        "\n",
        "**Operations:**\n",
        "\n",
        "* Reads the CSV file from the given path.\n",
        "* Prints the dataset shape (rows Ã— columns).\n",
        "* Returns the loaded DataFrame.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "df = load_data('breast_cancer.csv')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **2. `standardize_target(df, target_col)`**\n",
        "\n",
        "**Purpose:**\n",
        "Ensures that the target column (`Status`) is in a consistent numerical format for model training.\n",
        "\n",
        "**Operations:**\n",
        "\n",
        "* Converts â€œaliveâ€ â†’ 1 and â€œdeadâ€ â†’ 0.\n",
        "* Removes leading/trailing spaces and converts to lowercase for consistency.\n",
        "* Handles numeric targets (0/1) directly if already numeric.\n",
        "* Returns the updated DataFrame.\n",
        "\n",
        "**Why it matters:**\n",
        "Machine learning models require numerical target labels.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. `build_and_train(df, target_col='Status')`**\n",
        "\n",
        "**Purpose:**\n",
        "Main training pipeline â€” prepares data, encodes features, trains a model, evaluates it, and saves the pipeline.\n",
        "\n",
        "**Key Steps:**\n",
        "\n",
        "#### a. **Feature & Target Separation**\n",
        "\n",
        "Splits dataset into:\n",
        "\n",
        "* **X (features):** All columns except `Status`\n",
        "* **y (target):** The `Status` column\n",
        "\n",
        "#### b. **Feature Categorization**\n",
        "\n",
        "Identifies:\n",
        "\n",
        "* **Numeric columns:** age, tumor size, nodes examined, etc.\n",
        "* **Categorical columns:** race, T stage, N stage, estrogen status, etc.\n",
        "\n",
        "#### c. **Preprocessing Pipelines**\n",
        "\n",
        "Handles missing values and encoding:\n",
        "\n",
        "* **Numeric transformer:** Median imputation + StandardScaler\n",
        "* **Categorical transformer:** Most-frequent imputation + OneHotEncoder\n",
        "\n",
        "All combined using a `ColumnTransformer`.\n",
        "\n",
        "#### d. **Model Definition**\n",
        "\n",
        "Creates a `RandomForestClassifier` with 200 trees.\n",
        "\n",
        "#### e. **Full Pipeline**\n",
        "\n",
        "Combines preprocessing + model:\n",
        "\n",
        "```python\n",
        "pipe = Pipeline(steps=[('preproc', preprocessor), ('clf', clf)])\n",
        "```\n",
        "\n",
        "#### f. **Train-Test Split**\n",
        "\n",
        "Splits data into training (80%) and testing (20%) sets using `train_test_split`.\n",
        "\n",
        "#### g. **Training and Evaluation**\n",
        "\n",
        "* Fits model on training data.\n",
        "* Evaluates accuracy, ROC-AUC score, and prints the confusion matrix.\n",
        "* Displays a classification report (Precision, Recall, F1-Score).\n",
        "\n",
        "#### h. **Model Saving**\n",
        "\n",
        "Saves the entire preprocessing and model pipeline using:\n",
        "\n",
        "```python\n",
        "joblib.dump(pipe, 'bc_preproc_pipe.joblib')\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "Returns the trained pipeline and column lists for future predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. `prognosis_recommendation(pipe, patient_row, numeric_cols, categorical_cols, prob_threshold=0.5)`**\n",
        "\n",
        "**Purpose:**\n",
        "Provides an intelligent prognosis and risk recommendation for a new or existing patient.\n",
        "\n",
        "**Operations:**\n",
        "\n",
        "1. **Input Formatting:**\n",
        "   Converts a single patientâ€™s data (list, Series, or DataFrame) into the format expected by the trained model.\n",
        "\n",
        "2. **Prediction:**\n",
        "\n",
        "   * Predicts survival probability (`alive`) using `predict_proba()`.\n",
        "   * Predicts survival status (`alive`/`dead`) using `predict()`.\n",
        "\n",
        "3. **Risk Labeling:**\n",
        "\n",
        "   * If predicted survival probability < 0.5 â†’ â€œHigh Riskâ€.\n",
        "   * Else â†’ â€œLow/Moderate Riskâ€.\n",
        "\n",
        "4. **Feature Importance Extraction:**\n",
        "\n",
        "   * Retrieves top influential features (based on modelâ€™s feature_importances_).\n",
        "   * Helps explain which parameters contributed most.\n",
        "\n",
        "5. **Recommendation Text:**\n",
        "   Generates a human-readable prognosis message:\n",
        "\n",
        "   * **Very High Risk:** Urgent treatment needed.\n",
        "   * **High Risk:** Consult oncologist and run further diagnostics.\n",
        "   * **Moderate Risk:** Monitor closely, stage verification needed.\n",
        "   * **Low Risk:** Routine follow-up recommended.\n",
        "\n",
        "**Returns a dictionary like:**\n",
        "\n",
        "```python\n",
        "{\n",
        "  'predicted_class': 1,\n",
        "  'probability_alive': 0.82,\n",
        "  'risk_label': 'Low/Moderate risk',\n",
        "  'top_influential': [['Tumor Size', 0.18], ['Grade', 0.14], ...],\n",
        "  'recommendation': 'âœ… Low risk. Routine follow-up recommended.'\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **5. `__main__` (Program Entry Point)**\n",
        "\n",
        "**Purpose:**\n",
        "Runs the complete system when the script is executed directly.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Loads dataset.\n",
        "2. Trains model using `build_and_train()`.\n",
        "3. Selects one patient (row 0) as an example.\n",
        "4. Generates prognosis and prints the recommendation.\n",
        "5. Saves model and preprocessing pipeline to disk.\n",
        "\n",
        "**Example Output:**\n",
        "\n",
        "```\n",
        "Accuracy: 91.23%\n",
        "ROC AUC: 0.935\n",
        "Confusion Matrix:\n",
        "[[75 5]\n",
        " [3 89]]\n",
        "\n",
        "Example patient actual status: alive\n",
        "Recommendation:\n",
        "âœ… Low risk. Routine follow-up and standard of care monitoring recommended.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ˆ **Summary of What the Code Does**\n",
        "\n",
        "| Function / Step              | Description                                                     |\n",
        "| ---------------------------- | --------------------------------------------------------------- |\n",
        "| `load_data()`                | Loads dataset into memory                                       |\n",
        "| `standardize_target()`       | Converts target labels (alive/dead) into 0/1                    |\n",
        "| `build_and_train()`          | Handles full ML pipeline: preprocessing â†’ training â†’ evaluation |\n",
        "| `prognosis_recommendation()` | Generates individualized prognosis and treatment advice         |\n",
        "| `__main__`                   | Orchestrates training and demonstrates a sample prediction      |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to make a **short documentation paragraph** (for your project report) summarizing all these functions in 150â€“200 words (suitable for your â€œImplementationâ€ or â€œSystem Designâ€ section)?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgllPqi2_jAI"
      },
      "outputs": [],
      "source": [
        "# ------------------ TF-IDF (Theory + Intuition + Example) ------------------\n",
        "#\n",
        "# PURPOSE:\n",
        "# --------\n",
        "# TF-IDF stands for **Term Frequencyâ€“Inverse Document Frequency**.\n",
        "# It is a statistical method used to convert text data (like movie tags, genres, or descriptions)\n",
        "# into **numerical feature vectors** that can be used by machine learning models.\n",
        "#\n",
        "# WHY WE USE IT:\n",
        "# --------------\n",
        "# - In a recommendation system, we need a numeric representation of text content (e.g., movie tags or summaries).\n",
        "# - TF-IDF helps highlight *important and distinctive* words for each item (movie).\n",
        "# - It gives more weight to words that appear often in one movie but not across all movies.\n",
        "# - It reduces the influence of very common words like \"the\", \"movie\", \"film\", etc.\n",
        "#\n",
        "# CORE IDEA:\n",
        "# -----------\n",
        "# TF-IDF = Term Frequency (TF) Ã— Inverse Document Frequency (IDF)\n",
        "#\n",
        "# Mathematically:\n",
        "#   tfidf(t, d) = tf(t, d) * idf(t)\n",
        "#\n",
        "# where:\n",
        "#   tf(t, d)   = (Number of times term t appears in document d) / (Total terms in d)\n",
        "#   idf(t)     = log( N / (1 + df(t)) )\n",
        "#   N          = total number of documents (e.g., total movies)\n",
        "#   df(t)      = number of documents that contain term t\n",
        "#\n",
        "# EXPLANATION:\n",
        "# ------------\n",
        "# - TF (Term Frequency): Measures how frequently a word occurs in a single document.\n",
        "#   â†’ Higher TF means the term is important for that document.\n",
        "#\n",
        "# - IDF (Inverse Document Frequency): Measures how rare a word is across all documents.\n",
        "#   â†’ A term appearing in many documents is less useful for distinguishing them.\n",
        "#   â†’ The log ensures smoother scaling.\n",
        "#\n",
        "# - Multiplying them (TF Ã— IDF):\n",
        "#   â†’ High score if the term is frequent in one document but rare overall.\n",
        "#   â†’ Low score if the term is common across all documents.\n",
        "#IN CODE:\n",
        "# - Each movie becomes a vector of TF-IDF weights (one dimension per word).\n",
        "# - This TF-IDF matrix is then used with cosine similarity to find movies with similar content.\n",
        "\n",
        "# ------------------ COSINE SIMILARITY (theory + example) ------------------\n",
        "#\n",
        "# What it measures (intuition):\n",
        "# - Cosine similarity measures the angle between two vectors in high-dimensional space.\n",
        "# - It tells us how similar the *direction* of two vectors is, ignoring their magnitudes.\n",
        "# - For text (TF-IDF) vectors: two documents with similar words have a small angle -> cosine near 1.\n",
        "#\n",
        "# Formula (compact):\n",
        "#   cosine(a, b) = (a Â· b) / (||a|| * ||b||)\n",
        "#   where\n",
        "#     - a Â· b = sum_i (a_i * b_i)  (dot product)\n",
        "#     - ||a|| = sqrt(sum_i a_i^2)  (Euclidean norm)\n",
        "#\n",
        "# Properties:\n",
        "# - Range: for TF-IDF (non-negative) vectors cosine âˆˆ [0, 1] (0 = orthogonal/unrelated, 1 = identical direction).\n",
        "# - Insensitive to scale: if you multiply a vector by a positive constant, cosine doesn't change.\n",
        "#\n",
        "# Step-by-step numeric example:\n",
        "#   a = [1, 2, 3]\n",
        "#   b = [4, 5, 6]\n",
        "#   dot = 1*4 + 2*5 + 3*6 = 32\n",
        "#   ||a|| = sqrt(1^2 + 2^2 + 3^2) = sqrt(14) â‰ˆ 3.7417\n",
        "#   ||b|| = sqrt(4^2 + 5^2 + 6^2) = sqrt(77) â‰ˆ 8.7750\n",
        "#   cosine = 32 / (3.7417 * 8.7750) â‰ˆ 0.9746  (very similar)\n",
        "#\n",
        "# How it's used in this notebook:\n",
        "# - Compute TF-IDF vectors for movies (using tags/genres/descriptions).\n",
        "# - Cosine similarity between movie vectors => content-similarity matrix.\n",
        "# - For a user's liked movies, content-score of a candidate can be average or weighted sum of similarities.\n",
        "\n",
        "# ------------------ SVD / MATRIX FACTORIZATION (theory + training) ------------------\n",
        "#\n",
        "# Goal and intuition:\n",
        "# - Collaborative Filtering via matrix factorization tries to explain the user-item rating matrix R\n",
        "#   with low-dimensional latent factors. Each user and each item get a k-dimensional vector.\n",
        "# - The predicted rating is roughly the dot product between user and item vectors (plus biases).\n",
        "#\n",
        "# Mathematical view (full SVD vs learned MF):\n",
        "# - Full SVD (linear algebra): R = U Î£ V^T  (requires a fully observed matrix)\n",
        "\n",
        "# Practical hyperparameters:\n",
        "# - n_factors (k): dimensionality of latent space (common: 20â€“200).\n",
        "# - n_epochs: number of passes over training data.\n",
        "# - lr (Î³): learning rate for SGD â€” control the update size.\n",
        "# - reg (Î»): regularization strength â€” prevents overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gBRwcJR_jIP"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# NOTE: Paste this entire block into ONE Jupyter cell. It's all comments/explanations.\n",
        "# -----------------------------------------------------------------------------\n",
        "#\n",
        "# Overview of this notebook / pipeline (simple language)\n",
        "# -----------------------------------------------------\n",
        "# 1) Load and inspect dataset (user-item interactions, item metadata, maybe text fields).\n",
        "# 2) Clean / preprocess data (fill missing values, normalize, encode categories).\n",
        "# 3) Build item/content vectors (e.g., TF-IDF on product descriptions or tags).\n",
        "# 4) Optionally reduce dimensionality of those vectors with TruncatedSVD (faster, denser).\n",
        "# 5) Compute item-item similarity using cosine similarity on those vectors (content-based).\n",
        "# 6) Build an ML model (RandomForest) to predict a target (rating, click/purchase probability).\n",
        "# 7) Combine (hybrid) â€” e.g., use similarity scores + RandomForest predicted score to rank recommendations.\n",
        "# 8) Evaluate and visualize (precision@k, ROC/PR, similarity heatmaps, feature importances, recommended items).\n",
        "#\n",
        "# What \"hybrid\" recommender means (short)\n",
        "# ---------------------------------------\n",
        "# - Hybrid = combines two or more recommendation approaches. Commonly:\n",
        "#   * Content-based (use item features / text similarity)\n",
        "#   * Collaborative or ML-based (use user behaviour or a predictor like RandomForest)\n",
        "# - Benefit: content-based covers new items (cold start); ML/collaborative captures complex usage patterns.\n",
        "#\n",
        "# TF-IDF (purpose & short formula)\n",
        "# -------------------------------\n",
        "# - Purpose: convert textual item metadata (title, description, tags) into numeric vectors\n",
        "#   that reflect how important each word is for an item compared to the whole corpus.\n",
        "# - Simplified formula:\n",
        "#     TF-IDF(t, d) = TF(t, d) * IDF(t)\n",
        "#     TF(t, d) = frequency of term t in document d\n",
        "#     IDF(t) = log( N / (1 + df_t) )\n",
        "#       where N = total number of documents (items), df_t = number of documents containing term t\n",
        "# - Result: each item -> a sparse vector; similar items have similar TF-IDF vectors.\n",
        "#\n",
        "# Cosine similarity (what it measures + math)\n",
        "# -------------------------------------------\n",
        "# - Purpose: measure similarity (direction) between two vectors regardless of their length.\n",
        "# - Formula for two vectors a and b:\n",
        "#     cosine_sim(a, b) = (a Â· b) / (||a|| * ||b||)\n",
        "#   where (a Â· b) is dot-product and ||a|| is the vector norm (length).\n",
        "# - Range: [-1, 1] for general vectors, but with TF-IDF (non-negative) it's [0, 1].\n",
        "# - Use: compute item-item similarity matrix; for each item, retrieve top-N most similar items.\n",
        "#\n",
        "# Truncated SVD (why and what it does)\n",
        "# ------------------------------------\n",
        "# - Purpose: reduce dimensionality of sparse TF-IDF vectors into dense components (k components).\n",
        "# - Theory (matrix form): given item-term matrix A (items x terms),\n",
        "#     Truncated SVD gives A â‰ˆ U_k Î£_k V_k^T\n",
        "#   where U_k (items x k) are item embeddings, Î£_k is diagonal of singular values, V_k are term directions.\n",
        "# - Benefits:\n",
        "#   * faster similarity calculations,\n",
        "#   * removes noise, captures main latent topics,\n",
        "#   * works well when you want dense vectors for ML or nearest-neighbor search.\n",
        "#\n",
        "# Why RandomForest? (simple, practical reasons)\n",
        "# --------------------------------------------\n",
        "# - RandomForest is an ensemble of decision trees â€” each tree sees a random subset of data/features.\n",
        "# - Strengths relevant to a recommender ML component:\n",
        "#   * Handles mixed feature types (numerical + categorical after encoding).\n",
        "#   * Captures non-linear interactions between features (no linearity assumption).\n",
        "#   * Robust to outliers and moderately robust to overfitting thanks to averaging across trees.\n",
        "#   * Provides feature_importances_ so we can interpret which features matter.\n",
        "#   * Works well without heavy hyperparameter tuning as a strong baseline.\n",
        "# - Typical use in a recommender:\n",
        "#   * Predict user-item score (rating) or purchase probability (binary).\n",
        "#   * Use predicted score to rank candidate items for a user.\n",
        "#\n",
        "# How the hybrid ranking typically works (practical recipe)\n",
        "# --------------------------------------------------------\n",
        "# 1) Candidate generation (fast): for a given user, get a pool of candidate items:\n",
        "#    - Popular items, items similar to the ones the user liked (content-based via cosine),\n",
        "#      and any items the user has previously interacted with.\n",
        "# 2) Scoring candidates:\n",
        "#    - For each candidate, compute:\n",
        "#        * content_score = cosine similarity (or average similarity to user's liked items)\n",
        "#        * ml_score = RandomForest.predict_proba(...) or predicted rating\n",
        "#    - Normalize these scores to comparable range (e.g., MinMaxScaler to 0-1).\n",
        "# 3) Combine scores (example):\n",
        "#    final_score = alpha * ml_score + (1 - alpha) * content_score\n",
        "#    where alpha in [0,1] balances ML vs content. Tune alpha on validation set.\n",
        "# 4) Sort candidates by final_score and pick top-K recommendations.\n",
        "#\n",
        "# Quick explanation of common plots in the notebook\n",
        "# -------------------------------------------------\n",
        "# 1) Dataset preview / value counts / histograms\n",
        "#    - Shows distribution of users, items, ages, countries, rating counts.\n",
        "#    - Use to detect sparsity, skew, or odd values.\n",
        "#\n",
        "# 2) TF-IDF / SVD explained variance / scree plot\n",
        "#    - Scree plot of singular values (or cumulative explained variance) vs component index:\n",
        "#      * Shows how many components capture most of the signal.\n",
        "#      * If the curve flattens quickly, a small k is enough.\n",
        "#\n",
        "# 3) Cosine similarity heatmap (items x items)\n",
        "#    - Each cell (i, j) visualizes similarity between item i and item j.\n",
        "#    - Darker / higher values â†’ more similar.\n",
        "#    - Use for sanity-check: items of same category should show high similarity.\n",
        "#\n",
        "# 4) Top-N recommended items bar chart / table\n",
        "#    - For a sample user, shows top recommended items and their combined scores.\n",
        "#    - Explanation: higher bars = higher final_score (more recommended).\n",
        "#\n",
        "# 5) Feature importance (RandomForest)\n",
        "#    - A bar chart of feature_importances_ from the RandomForest.\n",
        "#    - Interpretation: features with larger values are more influential in predictions.\n",
        "#      * E.g., \"user_purchase_count\" high importance â†’ past activity is predictive.\n",
        "#\n",
        "# 6) Model metrics plots:\n",
        "#    - Confusion matrix (binary classification): shows TP / FP / TN / FN counts.\n",
        "#      * Use it to see types of errors (false positives vs false negatives).\n",
        "#    - ROC curve (binary): TPR vs FPR for different thresholds. AUC summarizes area under curve.\n",
        "#      * Closer to top-left = better. AUC=1 is perfect; 0.5 is random.\n",
        "#    - Precision-Recall curve: useful for imbalanced problems (purchase events are rare).\n",
        "#      * Precision = TP / (TP + FP). Recall = TP / (TP + FN).\n",
        "#    - Precision@K / Recall@K: for recommenders we care about top-K â€” how many of the top-K are relevant.\n",
        "#\n",
        "# 7) Score distribution histograms\n",
        "#    - Histograms of ml_score or final_score can show whether scores are concentrated near extremes.\n",
        "#    - If too many scores are near max/min, consider rescaling or rebalancing.\n",
        "#\n",
        "# Practical tips & common pitfalls\n",
        "# -------------------------------\n",
        "# - Cold start: new items with little behavior data rely on content-based (TF-IDF + similarity).\n",
        "# - Overfitting: RandomForest can overfit if trees are too deep or data leaks are present. Use validation.\n",
        "# - Leakage: Make sure features used for training are available at prediction time (no future info).\n",
        "# - Imbalanced targets: use stratified sampling or class_weight if predicting rare events (purchases).\n",
        "# - Evaluation: use time-based train/test splits for temporal recommendation tasks (simulate real-life).\n",
        "#\n",
        "# Minimal list of mapping between typical sklearn functions and pipeline steps:\n",
        "# ---------------------------------------------------------------------------\n",
        "# - Data load: pd.read_csv(...) or df.head()\n",
        "# - TF-IDF: sklearn.feature_extraction.text.TfidfVectorizer(...)\n",
        "# - Dimensionality reduction: sklearn.decomposition.TruncatedSVD(n_components=k)\n",
        "# - Similarity: sklearn.metrics.pairwise.cosine_similarity(matrix)\n",
        "# - Machine learning model: sklearn.ensemble.RandomForestClassifier or RandomForestRegressor\n",
        "# - Scaling: sklearn.preprocessing.MinMaxScaler()\n",
        "# - Evaluation: sklearn.metrics.roc_auc_score, precision_score, recall_score, confusion_matrix\n",
        "#\n",
        "# Example math reminders (copyable)\n",
        "# --------------------------------\n",
        "# - Cosine similarity: cos(a,b) = (aÂ·b) / (||a|| * ||b||)\n",
        "# - SVD: A â‰ˆ U_k Î£_k V_k^T  (keep k components; U_k are item embeddings)\n",
        "# - TF-IDF (component): IDF(t) = log( N / (1 + df_t) )\n",
        "#\n",
        "# Final short summary (one-liner)\n",
        "# --------------------------------\n",
        "# - This notebook uses a HYBRID approach: content-based similarity (TF-IDF + SVD + cosine) for item similarity\n",
        "#   and a RandomForest ML model to predict user-item preference; both outputs are combined (weighted) to\n",
        "#   produce final ranked recommendations. Visualizations validate similarity, model predictions, and ranking quality.\n",
        "#\n",
        "# -----------------------------------------------------------------------------\n",
        "# END OF COMMENTS - paste above into one Jupyter cell.\n",
        "# -----------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pdpwzKF_jYQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
