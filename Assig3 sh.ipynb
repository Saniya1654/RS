{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c3aab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Recommendations for user 1 and movie 'Toy Story (1995)':\n",
      "[np.str_(\"Devil's Own, The (1997)\"), np.str_('G-Force (2009)'), np.str_('Chaos (2005)'), np.str_('Royal Scandal, The (2001)'), np.str_('Other Man, The (2008)'), np.str_('Black Caesar (1973)'), np.str_('Raze (2013)'), 'Antz (1998)', 'Adventures of Rocky and Bullwinkle, The (2000)', 'Monsters, Inc. (2001)', np.str_('Descent (2007)'), 'Toy Story 2 (1999)', \"Emperor's New Groove, The (2000)\"]\n"
     ]
    }
   ],
   "source": [
    "# Hybrid Movie Recommendation System\n",
    "# Using TF-IDF (Content-Based) + SVD (Collaborative)\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, pairwise_distances\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "# --- Step 1: Load and clean dataset ---\n",
    "movies = pd.read_csv('RS-A2_A3_movie.csv')\n",
    "movies = movies[['title', 'genres']]\n",
    "movies.dropna(inplace=True)\n",
    "\n",
    "# Convert JSON-like or pipe-separated 'genres' column to plain text\n",
    "# (handles both ['Action', 'Drama'] or Action|Drama)\n",
    "def clean_genres(x):\n",
    "    try:\n",
    "        if isinstance(x, str):\n",
    "            if x.startswith('['):\n",
    "                return ' '.join([i['name'] if isinstance(i, dict) else i for i in ast.literal_eval(x)])\n",
    "            elif '|' in x:\n",
    "                return x.replace('|', ' ')\n",
    "            else:\n",
    "                return x\n",
    "        return ''\n",
    "    except Exception:\n",
    "        return ''\n",
    "\n",
    "movies['genres'] = movies['genres'].apply(clean_genres)\n",
    "movies['content'] = movies['genres']  \n",
    "\n",
    "# --- Step 2: Content-based TF-IDF similarity ---\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(movies['content'])\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# --- Step 3: Create sample user-rating matrix for collaborative filtering \n",
    "np.random.seed(42)\n",
    "user_ids = [1, 2, 3, 4, 5]\n",
    "sample_movies = movies['title'].sample(10, random_state=42).tolist()\n",
    "\n",
    "ratings_data = []\n",
    "for user in user_ids:\n",
    "    for title in np.random.choice(sample_movies, 5, replace=False):\n",
    "        ratings_data.append({\n",
    "            'user_id': user,\n",
    "            'title': title,\n",
    "            'rating': np.random.randint(3, 6)\n",
    "        })\n",
    "\n",
    "ratings_df = pd.DataFrame(ratings_data)\n",
    "\n",
    "# --- Step 4: Collaborative filtering using SVD ---\n",
    "user_item_matrix = ratings_df.pivot(index='user_id', columns='title', values='rating').fillna(0)\n",
    "user_item_sparse = csr_matrix(user_item_matrix.values)\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "latent_matrix = svd.fit_transform(user_item_sparse)\n",
    "\n",
    "# --- Step 5: Recommendation functions ---\n",
    "def get_content_based_recommendations(title, cosine_sim=cosine_sim):\n",
    "    \"\"\"Recommend similar movies based on TF-IDF cosine similarity.\"\"\"\n",
    "    if title not in movies['title'].values:\n",
    "        return []\n",
    "    idx = movies.index[movies['title'] == title][0]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:6]  # skip same movie, top 5 similar\n",
    "    return [movies['title'].iloc[i[0]] for i in sim_scores]\n",
    "\n",
    "def get_collaborative_recommendations(user_id):\n",
    "    \"\"\"Recommend movies based on user similarity using latent features.\"\"\"\n",
    "    if user_id not in user_item_matrix.index:\n",
    "        return []\n",
    "    user_idx = list(user_item_matrix.index).index(user_id)\n",
    "    similar_users = pairwise_distances(\n",
    "        latent_matrix[user_idx].reshape(1, -1), latent_matrix, metric='cosine'\n",
    "    )[0]\n",
    "    similar_users_indices = np.argsort(similar_users)[:3]\n",
    "    recommended_movies = []\n",
    "    for idx in similar_users_indices:\n",
    "        uid = list(user_item_matrix.index)[idx]\n",
    "        recommended_movies.extend(ratings_df[ratings_df['user_id'] == uid]['title'].tolist())\n",
    "    return list(set(recommended_movies))\n",
    "\n",
    "def hybrid_recommendations(user_id, title):\n",
    "    \"\"\"Combine content-based and collaborative filtering results.\"\"\"\n",
    "    content_based = get_content_based_recommendations(title)\n",
    "    collaborative_based = get_collaborative_recommendations(user_id)\n",
    "    combined = list(set(content_based + collaborative_based))\n",
    "    return combined\n",
    "\n",
    "# --- Step 6: Example Usage ---\n",
    "user_id = 1\n",
    "movie_title = movies['title'].iloc[0]  # pick any movie from dataset\n",
    "recommended_movies = hybrid_recommendations(user_id, movie_title)\n",
    "\n",
    "print(f\"Hybrid Recommendations for user {user_id} and movie '{movie_title}':\")\n",
    "print(recommended_movies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eaa532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview (high level)\n",
    "# - This notebook builds a HYBRID recommendation system that combines:\n",
    "#     1) Content-based filtering using TF-IDF on movie metadata (genres/text).\n",
    "#     2) Collaborative filtering using a low-rank approximation (Truncated SVD)\n",
    "#        of the user-item matrix to capture latent preferences.\n",
    "# - The hybrid approach merges the two sets of recommendations to get more\n",
    "#   robust suggestions (covers cold-start with content info and personalized\n",
    "#   tastes from collaborative signals).\n",
    "#\n",
    "# Code flow (what each major block does)\n",
    "# 1) Data loading & cleaning\n",
    "#    - Load the CSV of movies (title, genres). Drop missing rows.\n",
    "#    - Prepare/parse any text fields so they can be vectorized (e.g., convert\n",
    "#      genre lists or strings into a single text field if needed).\n",
    "#\n",
    "# 2) Content-based (TF-IDF) part\n",
    "#    - TF-IDF vectorizer (sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "#      transforms each movie's text (genres/description) into a numeric vector.\n",
    "#    - Purpose of TF-IDF:\n",
    "#        * TF (term frequency): measures how often a token appears in a doc.\n",
    "#        * IDF (inverse document frequency): downweights terms that appear\n",
    "#          in many documents (common words) and boosts rare but informative terms.\n",
    "#        * Formula (common form):\n",
    "#            tfidf(t, d) = tf(t, d) * log( N / (1 + df(t)) )\n",
    "#          where:\n",
    "#            - tf(t,d) is the term frequency of token t in document d\n",
    "#            - N is total number of documents\n",
    "#            - df(t) is number of documents containing token t\n",
    "#    - The result is a matrix: M_content (num_movies × num_features).\n",
    "#\n",
    "# 3) Similarity measure (cosine similarity)\n",
    "#    - Cosine similarity is used to measure closeness between two vectors.\n",
    "#    - Formula:\n",
    "#            cos(θ) = (A · B) / (||A|| * ||B||)\n",
    "#      where (A · B) is dot product and ||A|| is Euclidean norm of A.\n",
    "#    - In code:\n",
    "#        * sklearn.metrics.pairwise.linear_kernel(X, Y) returns dot products.\n",
    "#        * If vectors are TF-IDF (which are often L2-normalized), the dot product\n",
    "#          equals cosine similarity.\n",
    "#    - We use cosine similarity to find movies with similar TF-IDF vectors\n",
    "#      (i.e., similar genre/description fingerprints).\n",
    "#\n",
    "# 4) Collaborative part (Truncated SVD on user-item matrix)\n",
    "#    - Build a user-item matrix R (users × items) where entries are ratings\n",
    "#      or implicit feedback (e.g., play counts). This matrix is typically sparse.\n",
    "#    - Truncated SVD (sklearn.decomposition.TruncatedSVD) factorizes R into:\n",
    "#            R ≈ U_k Σ_k V_k^T\n",
    "#      where:\n",
    "#        - U_k (users × k) are user latent factors,\n",
    "#        - Σ_k (k × k) is diagonal with top k singular values,\n",
    "#        - V_k^T (k × items) are item latent factors.\n",
    "#    - TruncatedSVD computes a low-rank approximation capturing the main\n",
    "#      latent patterns (e.g., \"action-lovers\" vs \"romcom-lovers\" axes).\n",
    "#    - After decomposition, each user gets a k-dimensional representation.\n",
    "#      We can compute distances or similarities between users in this latent\n",
    "#      space to find similar users and recommend items they liked.\n",
    "#    - Note: TruncatedSVD works well for sparse matrices and is similar in\n",
    "#      spirit to PCA but works directly on sparse inputs.\n",
    "#\n",
    "# 5) Pairwise distances / neighbor selection\n",
    "#    - pairwise_distances or cosine distances on the user-factor matrix\n",
    "#      are used to find nearest neighbors in latent space.\n",
    "#    - Typical approach:\n",
    "#        * find top similar users to a target user,\n",
    "#        * aggregate their liked items that the target user hasn't seen,\n",
    "#        * rank and return those items.\n",
    "#\n",
    "# 6) Combining / hybrid logic\n",
    "#    - The function hybrid_recommendations(user_id, title) does:\n",
    "#        * get_content_based_recommendations(title) -> list of movies similar\n",
    "#          to the seed movie (by TF-IDF + cosine similarity).\n",
    "#        * get_collaborative_recommendations(user_id) -> list of movies from\n",
    "#          collaborative filtering (via SVD and nearest neighbors).\n",
    "#        * combine the two lists (e.g., set union or weighted merge) to return\n",
    "#          the final recommendations.\n",
    "#    - Combining strategies (common options):\n",
    "#        * union of top-N from both methods,\n",
    "#        * weighted scoring: score = α * content_score + (1-α) * collab_score,\n",
    "#        * fallback: if few collab recs exist (cold-start), rely more on content.\n",
    "#\n",
    "# Practical/function notes (mapping to code pieces you likely have)\n",
    "# - TfidfVectorizer(...):\n",
    "#     * common options: max_features to limit vocabulary, ngram_range, stop_words,\n",
    "#       norm='l2' to normalize vectors (makes linear_kernel ~ cosine similarity).\n",
    "# - linear_kernel(tfidf_matrix, tfidf_matrix) computes matrix of dot products\n",
    "#   (fast implementation). When tf-idf vectors are L2-normalized, linear_kernel\n",
    "#   = cosine similarity.\n",
    "# - csr_matrix: efficient sparse representation for the user-item matrix.\n",
    "# - TruncatedSVD(n_components=k).fit_transform(R) gives a low-dim embedding.\n",
    "# - pairwise_distances(embeddings, metric='cosine' or 'euclidean') finds\n",
    "#   user-to-user or item-to-item distances.\n",
    "#\n",
    "# Why hybrid helps (intuition)\n",
    "# - Content-based strengths: works without user history (cold-start items),\n",
    "#   explains why an item was recommended (shared genres/keywords).\n",
    "# - Collaborative strengths: captures taste patterns users don't articulate in\n",
    "#   metadata (latent features like \"dark humour\" vs \"feel-good pacing\").\n",
    "# - Hybrid mitigates the weaknesses of each when combined.\n",
    "#\n",
    "# Practical tips / improvements\n",
    "# - Normalize scores before combining (scale content and collaborative scores\n",
    "#   to [0,1] using MinMaxScaler or similar).\n",
    "# - Use more metadata (cast, director, plot summary) to enrich TF-IDF.\n",
    "# - Use implicit-feedback weighting (e.g., popularity, recency) for collaborative\n",
    "#   signals to improve recommendations.\n",
    "# - Experiment with the number of SVD components (k) — too small loses detail,\n",
    "#   too large keeps noise.\n",
    "#\n",
    "# Quick math summary (cheat-sheet)\n",
    "# - TF-IDF:\n",
    "#     tfidf(t, d) = tf(t, d) * log( N / (1 + df(t)) )\n",
    "# - Cosine similarity between vectors a and b:\n",
    "#     cos(a, b) = (a · b) / (||a|| * ||b||)\n",
    "# - SVD (matrix M):\n",
    "#     M = U Σ V^T  (full SVD)\n",
    "#     Truncated SVD keeps top-k: M ≈ U_k Σ_k V_k^T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ TF-IDF (Theory + Intuition + Example) ------------------\n",
    "#\n",
    "# PURPOSE:\n",
    "# --------\n",
    "# TF-IDF stands for **Term Frequency–Inverse Document Frequency**.\n",
    "# It is a statistical method used to convert text data (like movie tags, genres, or descriptions)\n",
    "# into **numerical feature vectors** that can be used by machine learning models.\n",
    "#\n",
    "# WHY WE USE IT:\n",
    "# --------------\n",
    "# - In a recommendation system, we need a numeric representation of text content (e.g., movie tags or summaries).\n",
    "# - TF-IDF helps highlight *important and distinctive* words for each item (movie).\n",
    "# - It gives more weight to words that appear often in one movie but not across all movies.\n",
    "# - It reduces the influence of very common words like \"the\", \"movie\", \"film\", etc.\n",
    "#\n",
    "# CORE IDEA:\n",
    "# -----------\n",
    "# TF-IDF = Term Frequency (TF) × Inverse Document Frequency (IDF)\n",
    "#\n",
    "# Mathematically:\n",
    "#   tfidf(t, d) = tf(t, d) * idf(t)\n",
    "#\n",
    "# where:\n",
    "#   tf(t, d)   = (Number of times term t appears in document d) / (Total terms in d)\n",
    "#   idf(t)     = log( N / (1 + df(t)) )\n",
    "#   N          = total number of documents (e.g., total movies)\n",
    "#   df(t)      = number of documents that contain term t\n",
    "#\n",
    "# EXPLANATION:\n",
    "# ------------\n",
    "# - TF (Term Frequency): Measures how frequently a word occurs in a single document.\n",
    "#   → Higher TF means the term is important for that document.\n",
    "#\n",
    "# - IDF (Inverse Document Frequency): Measures how rare a word is across all documents.\n",
    "#   → A term appearing in many documents is less useful for distinguishing them.\n",
    "#   → The log ensures smoother scaling.\n",
    "#\n",
    "# - Multiplying them (TF × IDF):\n",
    "#   → High score if the term is frequent in one document but rare overall.\n",
    "#   → Low score if the term is common across all documents.\n",
    "#IN CODE:\n",
    "# - Each movie becomes a vector of TF-IDF weights (one dimension per word).\n",
    "# - This TF-IDF matrix is then used with cosine similarity to find movies with similar content.\n",
    "\n",
    "# ------------------ COSINE SIMILARITY (theory + example) ------------------\n",
    "#\n",
    "# What it measures (intuition):\n",
    "# - Cosine similarity measures the angle between two vectors in high-dimensional space.\n",
    "# - It tells us how similar the *direction* of two vectors is, ignoring their magnitudes.\n",
    "# - For text (TF-IDF) vectors: two documents with similar words have a small angle -> cosine near 1.\n",
    "#\n",
    "# Formula (compact):\n",
    "#   cosine(a, b) = (a · b) / (||a|| * ||b||)\n",
    "#   where\n",
    "#     - a · b = sum_i (a_i * b_i)  (dot product)\n",
    "#     - ||a|| = sqrt(sum_i a_i^2)  (Euclidean norm)\n",
    "#\n",
    "# Properties:\n",
    "# - Range: for TF-IDF (non-negative) vectors cosine ∈ [0, 1] (0 = orthogonal/unrelated, 1 = identical direction).\n",
    "# - Insensitive to scale: if you multiply a vector by a positive constant, cosine doesn't change.\n",
    "#\n",
    "# Step-by-step numeric example:\n",
    "#   a = [1, 2, 3]\n",
    "#   b = [4, 5, 6]\n",
    "#   dot = 1*4 + 2*5 + 3*6 = 32\n",
    "#   ||a|| = sqrt(1^2 + 2^2 + 3^2) = sqrt(14) ≈ 3.7417\n",
    "#   ||b|| = sqrt(4^2 + 5^2 + 6^2) = sqrt(77) ≈ 8.7750\n",
    "#   cosine = 32 / (3.7417 * 8.7750) ≈ 0.9746  (very similar)\n",
    "#\n",
    "# How it's used in this notebook:\n",
    "# - Compute TF-IDF vectors for movies (using tags/genres/descriptions).\n",
    "# - Cosine similarity between movie vectors => content-similarity matrix.\n",
    "# - For a user's liked movies, content-score of a candidate can be average or weighted sum of similarities.\n",
    "\n",
    "# ------------------ SVD / MATRIX FACTORIZATION (theory + training) ------------------\n",
    "#\n",
    "# Goal and intuition:\n",
    "# - Collaborative Filtering via matrix factorization tries to explain the user-item rating matrix R\n",
    "#   with low-dimensional latent factors. Each user and each item get a k-dimensional vector.\n",
    "# - The predicted rating is roughly the dot product between user and item vectors (plus biases).\n",
    "#\n",
    "# Mathematical view (full SVD vs learned MF):\n",
    "# - Full SVD (linear algebra): R = U Σ V^T  (requires a fully observed matrix)\n",
    "\n",
    "# Practical hyperparameters:\n",
    "# - n_factors (k): dimensionality of latent space (common: 20–200).\n",
    "# - n_epochs: number of passes over training data.\n",
    "# - lr (γ): learning rate for SGD — control the update size.\n",
    "# - reg (λ): regularization strength — prevents overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (genv)",
   "language": "python",
   "name": "genv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
